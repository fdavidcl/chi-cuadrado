%%
% Modificación de una plantilla de Latex para adaptarla al castellano.
%%

%%%%%%%%%%%%%%%%%%%%%
% Thin Sectioned Essay
% LaTeX Template
% Version 1.0 (3/8/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original Author:
% Nicolas Diaz (nsdiaz@uc.cl) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 10pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)
%\usepackage{helvet}
%\renewcommand{\familydefault}{\sfdefault}
\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage[usenames,dvipsnames]{color} % Coloring code
\usepackage{wrapfig} % Allows in-line images
\usepackage[utf8]{inputenc}
\usepackage{enumerate}
\usepackage{enumitem}

% Imágenes
\usepackage{graphicx}

\usepackage{amsmath}
% para importar svg
%\usepackage[generate=all]{svgfig}

% sudo apt-get install texlive-lang-spanish
\usepackage[spanish]{babel} % English language/hyphenation
\selectlanguage{spanish}
% Hay que pelearse con babel-spanish para el alineamiento del punto decimal
\decimalpoint
\usepackage{dcolumn}
\newcolumntype{d}[1]{D{.}{\esperiod}{#1}}
\makeatletter
\addto\shorthandsspanish{\let\esperiod\es@period@code}
\makeatother

\usepackage{longtable}
\usepackage{tabu}
\usepackage{supertabular}

\usepackage{multicol}
\newsavebox\ltmcbox

% Para algoritmos
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm}

% Para matrices
\usepackage{amsmath}

% Símbolos matemáticos
\usepackage{amssymb}
\usepackage{accents}
\let\oldemptyset\emptyset
\let\emptyset\varnothing

\usepackage[hidelinks]{hyperref}

\usepackage[section]{placeins} % Para gráficas en su sección.
\usepackage[T1]{fontenc} % Required for accented characters
\usepackage{tikz}
\newenvironment{allintypewriter}{\ttfamily}{\par}
\setlength{\parindent}{0pt}
\parskip=8pt
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography
\newcommand{\figura}[2]{\begin{figure}\centering \includegraphics[width=90mm]{#1} \caption{#2} \end{figure}}

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{center} % Center align
{\Huge\@title} % Increase the font size of the title
\end{center}

\vspace{20pt} % Some vertical space between the title and author name

\begin{flushright} % Right align
{\large\@author} % Author name
\\\@date % Date

\vspace{40pt} % Some vertical space between the author block and abstract
\end{flushright}
\renewcommand{\baselinestretch}{0.5}

}
%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{Distribución $\chi$-cuadrado}\\ % Title
\vspace{20 pt}
} % Subtitle

\author{\textsc{Daniel López\\
David Charte} % Author
\\{\textit{Universidad de Granada}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------
%\setcounter{secnumdepth}{3}
%\usepackage{anysize}
%\marginsize{3cm}{3cm}{2.5cm}{2.5cm}

\newtheorem{theorem}{Teorema}[section]
\newtheorem{definition}{Definición}[section]
\newtheorem{property}{Propiedad}[section]


\begin{document}
\maketitle
\tableofcontents
\setcounter{page}{1}
\pagebreak

\section{Definición}
\subsection{Función de densidad}
La distribución $\chi$-cuadrado es un caso particular de la distribución Gamma. Recordamos la función de densidad de esta distribución:

\begin{definition}[Distribución Gamma]
  Decimos que la variable aleatoria $X$ sigue una distribución Gamma si su función de densidad es:
  $$f(x\mid \alpha, \beta) = \frac 1 {\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-\frac x \beta},\quad x\in[0,+\infty[,\quad \alpha,\beta>0~.$$
\end{definition}

Ahora, si $2\alpha$ es un natural, llamamos $p=2\alpha$ (los grados de libertad) y evaluamos en $\beta=2$, nos queda la siguiente expresión:
$$\frac 1 {\Gamma(\frac p 2)2^{\frac p 2}}x^{\frac p 2-1}e^{-\frac x 2}~.$$

\begin{definition}[Distribución $\chi$-cuadrado]
  Decimos que la variable aleatoria $X$ sigue una distribución $\chi$-cuadrado si su función de densidad es:
  $$f(x\mid p) = \frac 1 {\Gamma(\frac p 2)2^{\frac p 2}}x^{\frac p 2-1}e^{-\frac x 2},\quad x\in[0,+\infty[,\quad p\in\{1,2,\dots\}~.$$
\end{definition}

\figura{pdf.png}{Función de densidad de la distribución $\chi$-cuadrado. Imagen de Wikipedia (CC BY).}

\subsection{Propiedades}

\begin{property}
  La suma de variables aleatorias con distribución $\chi$-cuadrado sigue una distribución $\chi$-cuadrado.
\end{property}

\begin{property}
  La suma de los cuadrados de variables aleatorias con distribución normal 0-1 sigue una distribución $\chi$-cuadrado.
\end{property}

\section{Función generatriz de momentos}
% La función generatriz de momentos se define como sigue:
% $$\phi(t)=E[e^{-tX}]=\sum_{x=0}^{\infty}e^{-t x}\frac{e^{-\theta}\theta^x}{x!}=e^{-\theta}\sum_{x=0}^{\infty}e^{t x}\frac{\theta^x}{x!}=e^{-\theta}\sum_{x=0}^{\infty}\frac{(\theta e^t)^x}{x!}=e^{-\theta}e^{\theta e^t}=e^{\theta(e^t-1)}$$
$$M_X(t)=\left(\frac 1 {1 - 2t}\right)^{\frac p 2},\quad t<\frac 1 2$$

\subsection{Esperanza}
% Para el cálculo de la esperanza tan solo debemos derivar la función generatriz de momentos una vez y evaluarla con t=0:
% $$\frac{\partial\phi}{\partial t} = \theta e^t e^{\theta(e^t-1)}$$
% Evaluamos la expresión en t=0 y tenemos que:
$$\mathrm E[X]=p$$

\subsection{Varianza}
% La varianza se puede expresar como $\sigma^2=E[X^2]-E[X]^2$ por lo que es tan solo calcular el momento de orden 2 usando la función generatriz de momentos, por lo que volvemos a derivar la expresión anterior:
% $$\frac{\partial^2\phi}{\partial t^2} = \theta e^t e^{\theta(e^t-1)}+\theta^2 e^{2t} e^{\theta(e^t-1)}$$
% Evaluamos la expresión en t=0 y tenemos que:
% $$E[X^2]=\theta+\theta^2$$
% Y por lo tanto la expresión de la varianza es la siguiente:
$$\mathrm{Var}[X]=2p$$

\section{Estimador máximo verosimil}
% Para una muestra de tamaño $n$, que denotamos por $x=(x_1,...,x_n)$ definimos la función de verosimilitud como:
% $$l(x_1,...,x_n|\theta)=\prod_{i=1}^{n} \frac{e^{-\theta}\theta^{x_i}}{x_i!}=e^{-n\theta}\prod_{i=1}^{n} \frac{\theta^{x_i}}{x_i!}$$
% Para el cálculo del estimador máximo verosimil de la ley de Poisson vamos a maximizar la función $log(l(x_1,...,x_n|\theta))$ puesto que el logaritmo es creciente conserva los máximos:
% $$L(x_1,...,x_n|\theta)=log(l(x_1,...,x_n|\theta))=-n\theta + \sum_{i=1}^{n}log(\frac{\theta^{x_i}}{x_i!})=-n\theta+ \sum_{i=1}^{n}(x_i log(\theta)-log(x_i!))$$
% Ahora calculamos la derivada para maximizar el logaritmo de la función de verosimilitud:
% $$\frac{\partial L}{\partial \theta}=-n+\sum_{i=1}^{n}\frac{x_i}{\theta}=-n+\frac{1}{\theta}\sum_{i=1}^{n}x_i$$
% Encontramos el extremo igualando a 0:
% $$-n+\frac{1}{\theta}\sum_{i=1}^{n}x_i=0 \Leftrightarrow \frac{1}{\theta}\sum_{i=1}^{n}x_i=n \Leftrightarrow \theta =\frac{\sum_{i=1}^{n}x_i}{n}$$
% Para verificar que hemos encontrado un máximo calculamos la segunda derivada y evaluamos en $\theta =\frac{\sum_{i=1}^{n}x_i}{n}$:
% $$\frac{\partial^2 L}{\partial \theta^2}=-\frac{1}{\theta^2}\sum_{i=1}^{n}x_i$$
% $$\frac{\partial^2 L}{\partial \theta^2}(\frac{\sum_{i=1}^{n}x_i}{n})<0$$
% Se tiene que la segunda derivada es negativa ya que la suma de variables $x_i$ es siempre positiva y $\theta$ está elevado al cuadrado por lo que siempre es positiva, por lo que no hay ningún valor que haga positiva la segunda derivada y por tanto $\hat{\theta}=\frac{\sum_{i=1}^{n}x_i}{n}$ es un estimador máximo verosimil para la ley de Poisson.

\subsection{Insesgadez}
\begin{definition}[Insesgadez]
Se denomina sesgo de un estimador a la diferencia entre la esperanza del estimador y el verdadero valor del parámetro a estimar. Un estimador es insesgado si su sesgo es nulo por ser su esperanza igual al parámetro que se desea estimar.
\end{definition}

% Sean n experimentos $x=(x_1,...x_n)$ independientes que siguen una distribución de Poisson de parámetro $\theta$, para que el estimador $\hat{\theta}$ sea insesgado tenemos que probar que:
% $$E[\hat{\theta}]=\theta$$
% \begin{proof}
% $$E[\hat{\theta}]=E[\frac{1}{n}\sum_{i=1}^{n}x_i]=\frac{1}{n}\sum_{i=1}^{n}E[x_i]=\frac{1}{n}\sum_{i=1}^{n}\theta=\frac{n\theta}{n}=\theta$$
% \end{proof}

\subsection{Eficiencia}
\begin{definition}[Eficiencia]
Un estimador $\hat{\theta}_1$ se dice que es más eficiente que otro estimador $\hat{\theta}_2$, si la varianza del primero es menor que la del segundo $Var(\hat{\theta}_1)<Var(\hat{\theta}_2)$.
\end{definition}

% Sean n experimentos $x=(x_1,...x_n)$ independientes que siguen una distribución de Poisson de parámetro $\theta$, para que el estimador $\hat{\theta}$ sea eficiente tenemos que probar que:
% $$Var(\hat{\theta})=\frac{1}{nI(\theta)}$$
% Se verifica que la varianza del estimador alcanza la cota de Cramér-Rao.
%
% \begin{proof}
% Calculamos la función información de Fisher para ello en primer lugar veamos cual es el valor del logaritmo de la función de densidad:
% $$log(f(x|\theta))=-\theta + xlog(\theta) -log(x!)$$
% Derivamos y elevamos al cuadrado previamente para el cálculo de la información de Fisher:
% $$(\frac{\partial}{\partial\theta}log(f(x|\theta)))^2=(\frac{x-\theta}{\theta})^2$$
% Calculamos la esperanza de lo anterior:
% $$I(\theta)=E[(\frac{\partial}{\partial\theta}log(f(x|\theta)))^2]=E[(\frac{x-\theta}{\theta})^2]=\frac{1}{\theta^2}E[(x-\theta)^2]=\frac{1}{\theta^2}Var(x)=\frac{\theta}{\theta^2}=\frac{1}{\theta}$$
% Puesto que tenemos $n$ experimentos:
% $$nI(\theta)=\frac{n}{\theta}$$
% Calculamos la varianza del estimador para ver si coincide con la información de Fisher:
% $$Var(\hat{\theta})=Var(\frac{\sum_{i=1}^{n}x_i}{n})=\frac{1}{n^2}Var(\sum_{i=1}^{n}x_i)=\frac{1}{n^2}\sum_{i=1}^{n}Var(x_i)=\frac{n\theta}{n^2}=\frac{\theta}{n}$$
% Por lo que se verifica la condición de de eficiencia y el estimador es eficiente.
% \end{proof}

\subsection{Consistencia}
\begin{theorem}
Una condición suficiente para que $\hat{\theta}$ sea un estimador consistente es que dicho estimador tiene que verificar:
\begin{description}
\item $E[\hat{\theta}] \rightarrow \theta$
\item $Var(\hat{\theta}) \rightarrow 0$
\end{description}
Con $n \rightarrow \infty$.
\end{theorem}

% Sean n experimentos $x=(x_1,...x_n)$ independientes que siguen una distribución de Poisson de parámetro $\theta$, el estimador $\hat{\theta}$ es consistente ya que sabemos que dicho estimador es insesgado, y conocemos al valor de la varianza del mismo:
% $$E[\hat{\theta}]=\theta$$
% $$Var(\hat{\theta})=\frac{\theta}{n} \rightarrow_{P_\theta} 0, \ \ \ n\rightarrow \infty$$

\subsection{Suficiencia}

\begin{theorem}
Sean $X_1, X_2, ..., X_n$ variables aleatorias independientes con una función distribución de distribución conjunta $f(x_1, x_2, ..., x_n | \theta)$ que depende del parámetro $\theta$. \\
Entonces se dice que el estadístico $u(x_1, ..., x_n)$ es suficiente para $\theta$ si y solamente si $f(x_1, x_2, ..., x_n | \theta)$ se puede factorizar de la siguiente forma: \\
$$ f(x_1,...,x_n| \theta) = \Phi(u(x_1, ..., x_n) | \theta)\cdot h(x_1, ..., x_n) $$
\end{theorem}

% Veamos ahora que el estadístico \(T(X) =\sum_{i=1}^n x_i\) es un estadístico suficiente para el parámetro $\theta$ de la distribución de Poisson: \\
% Consideremos en primer lugar la probabilidad conjunta de la distribución:
% $$P(X=x) = P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n) $$
% Teniendo en cuenta que en nuestra distribución de Poisson se tiene que: \(P(X=k) = \frac{e^{-\theta }\theta ^{k}}{k!}\) y que las observaciones son independientes podemos reescribirlo como:
% $$P(X=x) = \prod_{i=1}^nP(X_i = x_i) = \frac{e^{-\theta }\theta ^{x_1}}{x_1!}\cdot \frac{e^{-\theta }\theta ^{x_2}}{x_2!}\cdot ... \cdot \frac{e^{-\theta }\theta ^{x_n}}{x_n!}$$
% Simplificando tenemos:
% $$f(x_1...x_n | \theta) = \prod_{i=1}^nP(X_i = x_i) = e^{-n\theta }\theta ^{\sum_{i=1}^nx_i} \frac{1}{\prod_{i=1}^nx_i!}$$
% Donde vemos que hemos conseguido factorizar $f(x_1...x_n | \theta)$ como en el teorema; en nuestro caso $u(X) = T(X) = \sum_{i=1}^n x_i$ y $h(X) =  \frac{1}{\prod_{i=1}^nx_i!}$\\
% Para finalizar conviene resaltar que hemos probado que el estadístico $T(X)$ es suficiente para $\theta$, pero nosotros lo que queremos es ver que $\hat{\theta}$ es suficiente para $\theta$.\\
% Pero esto es trivial ya que $\hat{\theta} = \frac{1}{n}\sum_{i=1}^n x_i= \frac{1}{n}T(X)$ y podemos encontrar una nueva función $\hat u(x_1,...,x_n)$ tal que:
% $$f(x_1...x_n | \theta) = \prod_{i=1}^nP(X_i = x_i) = e^{-n\theta }\theta ^{n\frac{\sum_{i=1}^nx_i}{n}} \frac{1}{\prod_{i=1}^nx_i!}$$
% Que cumple el esquema: \\
% $$ f(x_1,...,x_n| \theta) = \Phi(u(\hat\theta) | \theta)\cdot h(x_1, ..., x_n) $$
%
% Donde: $\Phi(u(\hat\theta) | \theta) = \prod_{i=1}^nP(X_i = x_i) = e^{-n\theta }\theta ^{n\frac{\sum_{i=1}^nx_i}{n}}$ \\
% Y $h(x_1, ..., x_n) = \frac{1}{\prod_{i=1}^nx_i!}$

% \section{Ejemplos}
% \textbf{Calcular el número de particulas $\alpha$ emitidas en una fuente radiactiva: \\
% }Experimentalmente se obtiene que el numero medio de particulas $\alpha$ emitidas en dos minutos es 3.
% ¿Cual es la probabilidad de que en los próximos dos minutos el número de partículas $\alpha$ sea 0, 1, 2, 3 ó 4?\\
% ¿Y de que sea mayor o igual que cinco?\\
% \textbf{Solución:}\\
% La probabilidad de que $x$ partículas se emitan en un periodo de dos minutos se puede modelar mediante una función de distribución de Poisson:
% $$ P_{\mu}(x) =\frac{\mu^x}{x!}e^{-\mu} $$
% Donde $\mu$ es el valor promedio de particulas emitido en dos minutos.\\
% En nuestro caso $\mu = 1.5\cdot2 = 3$
% Calculemos ahora $P_{\mu}(i)$ para $i=0,1,2,3,4$\\
% $$P_{\mu}(0) = \frac{3^0}{0!}e^{-3} = e^{-3} = 0.05 $$
% $$P_{\mu}(1) = \frac{3^1}{1!}e^{-3} = 3e^{-3} = 0.15 $$
% $$P_{\mu}(2) = \frac{3^2}{2!}e^{-3} = \frac{9}{2}e^{-3} = 0.22 $$
% $$P_{\mu}(3) = \frac{3^3}{3!}e^{-3} = \frac{9}{2}e^{-3} = 0.22 $$
% $$P_{\mu}(4) = \frac{3^4}{4!}e^{-3} = \frac{9}{2}e^{-3} = 0.17 $$
% $$\sum_{i=5}^{+\infty}P_{\mu}(i) = \sum_{i=5}^{+\infty}\frac{3^i}{i!}e^{-3} = \sum_{i=0}^{+\infty}\frac{3^i}{i!}e^{-3} - \sum_{i=0}^{5}\frac{3^i}{i!}e^{-3} = e^3e^{-3} - \sum_{i=0}^{5}\frac{3^i}{i!}e^{-3} = 1 - 0.05 - 0.15 - 0.22 - 0.22 - 0.17 = 0.19$$
\end{document}
